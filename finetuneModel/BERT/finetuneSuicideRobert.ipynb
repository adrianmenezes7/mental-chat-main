{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict, load_metric,load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We must tokenise the input before feeding to the model. We use the tokenizer from the pretrained model.\n",
    "- The dataset must have the label column named as 'label', to be trained using Trainer API. It is preferable to keep the text column as 'text'. However since we are tokenising the text column, it can be named anything, as we just pass the tokenised columns input_ids and attention_mask to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for huggingface pipeline to give text labels instead of numbers\n",
    "id2label = {0: \"suicide\", 1:\"non-suicide\"}\n",
    "label2id = {\"suicide\":0, \"non-suicide\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_name = \"PHR_Suicide_Prediction_Roberta_Cleaned_Light\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2,label2id=label2id,id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vibhorag101/roberta-base-suicide-prediction-phr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniseDataset(dataset):\n",
    "    return(tokeniser(dataset[\"text\"],padding=\"max_length\",truncation=True))\n",
    "\n",
    "def convertLabel2ID(dataset):\n",
    "    dataset['label'] = label2id[dataset['label']]\n",
    "    return dataset\n",
    "    \n",
    "dataset = dataset.map(convertLabel2ID) \n",
    "tokenisedDataset = dataset.map(tokeniseDataset,batched=True)\n",
    "\n",
    "trainTokeniseDataset = tokenisedDataset[\"train\"]\n",
    "tempTokenisedDataset= tokenisedDataset[\"test\"]\n",
    "tempTokenisedDataset = tempTokenisedDataset.train_test_split(test_size=0.5)\n",
    "testTokenisedDataset = tempTokenisedDataset[\"train\"]\n",
    "valTokenisedDataset = tempTokenisedDataset[\"test\"]\n",
    "# print(trainTokeniseDataset)\n",
    "# print(valTokenisedDataset)\n",
    "# print(testTokenisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_rec = evaluate.load(\"recall\")\n",
    "    metric_pre = evaluate.load(\"precision\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "    # here the model just give logits, labels are passed from test/val dataset\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # for binary classification we can just take argmax of logits, but for multi-class classification we need to use softmax\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    recall = metric_rec.compute(predictions=predictions, references=labels)\n",
    "    precision = metric_pre.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"huggingface\", entity=\"vibhor20349\", name=finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=finetune_model_name,\n",
    "    report_to = 'wandb',\n",
    "    learning_rate=2e-5, # recommended in roberta paper\n",
    "    num_train_epochs=3, #recommended in bert paper\n",
    "    per_device_train_batch_size=16, # recommended in roberta paper\n",
    "    per_device_eval_batch_size=16, # recommended in roberta paper\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True #to clone the training repo just before starting to train, so push to hub works\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here tokeniser is passed to the trainer, but it is for only pushing to hub and not for tokenising the dataset\n",
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainTokeniseDataset,\n",
    "    eval_dataset=valTokenisedDataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokeniser,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3,early_stopping_threshold=0.001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainer.predict(testTokenisedDataset).metrics)\n",
    "trainer.evaluate(eval_dataset=testTokenisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "#trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1621, -0.1936]])\n",
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "### IF we want to make predictions\n",
    "input_text = \"Your input text goes here.\"\n",
    "inputs = tokeniser(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    ## BERT expects the keyword agruments \"token_type_ids\" and \"attention_mask\" in input.\n",
    "    # so we convert inputs dictionary to keyword arguments using ** before passing to the model\n",
    "    outputs = model(**inputs)\n",
    "    ## output of Bert contains logits for each class, pooled output and hidden states and attentions\n",
    "\n",
    "# Extract the predicted logits(raw values for each class)\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "## logits are just raw values for each class. To get probabilities we use softmax\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
